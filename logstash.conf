input {
  beats {  # beats input plugin mediante el cual logstash recibe los datos
    port => 5044
  }
}

filter {
  if [fields][source] == "syslog" { # aplica el filtro si el campo source es syslog
    grok { # grok es un plugin que permite hacer parsing de logs
      match => { "message" => "%{SYSLOGLINE}" } # se especifica el formato del log a parsear
    }
  } else if [fields][source] == "nginx" { 
    grok {
      match => { "message" => "%{COMBINEDAPACHELOG}" } 
    }
  } else if [fields][source] == "auth" {
    grok {
      match => { "message" => "%{DATA:auth_message}" }
    }
  } else if [fields][source] == "apache" {
    grok {
      match => { "message" => "%{COMBINEDAPACHELOG}" }
    }
  } else if [fields][source] == "custom_app" {
    grok {
      match => { "message" => "%{DATA:custom_message}" }
    }
  }
}

output { # output plugin que envía los datos a elasticsearch, define la salida de datos
  elasticsearch { 
    hosts => ["http://elasticsearch:9200"] # se especifica la dirección de elasticsearch nuevamente
    index => "logs-%{[fields][source]}-%{+yyyy.MM.dd}" # se especifica el índice donde se almacenarán los datos utilizando el campo fields.source y la fecha actual
    user => "elastic"
    action => "create" # se especifica la acción a realizar, en este caso crear un nuevo índice, sin este parámetro no se crean los índices
    password => "${ELASTIC_PASSWORD}"
  }
  stdout { codec => rubydebug } # se especifica que se imprima en consola los datos que se están enviando a elasticsearch
}